{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c28bSQkcLRKn"
      },
      "source": [
        "# Language Models\n",
        "## Homework 1: Language Modeling\n",
        "\n",
        "**Instructor**: Pavlos Protopapas<br />\n",
        "**Maximum Score**: 70\n",
        "\n",
        "<hr style=\"height:2pt\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "inKAshSLL7U1"
      },
      "source": [
        "\n",
        "## INSTRUCTIONS\n",
        "\n",
        "- This homework is a notebook. Download and work on it on your local machine or work on it in Colab.\n",
        "\n",
        "- This homework should be submitted in pairs.\n",
        "\n",
        "- Ensure you and your partner together have submitted the homework only once. Multiple submissions of the same work will be penalised and will cost you 2 points.\n",
        "\n",
        "- Please restart the kernel and run the entire notebook again before you submit.\n",
        "\n",
        "- Running cells out of order is a common pitfall in Jupyter Notebooks. To make sure your code works restart the kernel and run the whole notebook again before you submit. \n",
        "\n",
        "- To submit the homework, either one of you upload the working notebook on edStem and click the submit button on the bottom right corner.\n",
        "\n",
        "- Submit the homework well before the given deadline. Submissions after the deadline will not be graded.\n",
        "\n",
        "- We have tried to include all the libraries you may need to do the assignment in the imports statement at the top of this notebook. We strongly suggest that you use those and not others as we may not be familiar with them.\n",
        "\n",
        "- Comment your code well. This would help the graders in case there is any issue with the notebook while running. It is important to remember that the graders will not troubleshoot your code. \n",
        "\n",
        "- Please use .head() when viewing data. Do not submit a notebook that is **excessively long**. \n",
        "\n",
        "- In questions that require code to answer, such as \"calculate the $R^2$\", do not just output the value from a cell. Write a `print()` function that includes a reference to the calculated value, **not hardcoded**. For example: \n",
        "```\n",
        "print(f'The R^2 is {R:.4f}')\n",
        "```\n",
        "- Your plots should include clear labels for the $x$ and $y$ axes as well as a descriptive title (\"MSE plot\" is not a descriptive title; \"95 % confidence interval of coefficients of polynomial degree 5\" is).\n",
        "\n",
        "- **Ensure you make appropriate plots for all the questions it is applicable to, regardless of it being explicitly asked for.**\n",
        "\n",
        "<hr style=\"height:2pt\">"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Names of the people who worked on this homework together**\n",
        "#### / Names here/"
      ],
      "metadata": {
        "id": "oDJ0FICOrxoy"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkQ9H3dgXimi"
      },
      "source": [
        "## **HOMEWORK QUIZ**\n",
        "\n",
        "**For each part of the homework, there is an associated quiz on edStem. You are required to attempt that after completing each section of this homework.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2p5Wx_BMPUu"
      },
      "source": [
        "## **Setup Notebook**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gOe-gtbMVD5"
      },
      "source": [
        "**Imports**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWsstoz5KnwW"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import re\n",
        "import os\n",
        "import zipfile\n",
        "import collections\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import urllib.request\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "%matplotlib inline\n",
        "from IPython.core.display import HTML\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.python.keras import backend as K\n",
        "from tensorflow.keras.models import Model\n",
        "from collections import defaultdict\n",
        "\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCKMuo0TM5dH"
      },
      "source": [
        "**Verify Setup**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5Eh4YpOQTlW"
      },
      "outputs": [],
      "source": [
        "# Enable/Disable Eager Execution\n",
        "# Reference: https://www.tensorflow.org/guide/eager\n",
        "# TensorFlow's eager execution is an imperative programming environment that evaluates operations immediately, \n",
        "# without building graphs\n",
        "\n",
        "#tf.compat.v1.disable_eager_execution()\n",
        "#tf.compat.v1.enable_eager_execution()\n",
        "\n",
        "print(\"tensorflow version\", tf.__version__)\n",
        "print(\"keras version\", tf.keras.__version__)\n",
        "print(\"Eager Execution Enabled:\", tf.executing_eagerly())\n",
        "\n",
        "# Get the number of replicas \n",
        "strategy = tf.distribute.MirroredStrategy()\n",
        "print(\"Number of replicas:\", strategy.num_replicas_in_sync)\n",
        "\n",
        "devices = tf.config.experimental.get_visible_devices()\n",
        "print(\"Devices:\", devices)\n",
        "print(tf.config.experimental.list_logical_devices('GPU'))\n",
        "\n",
        "print(\"GPU Available: \", tf.config.list_physical_devices('GPU'))\n",
        "print(\"All Physical Devices\", tf.config.list_physical_devices())\n",
        "\n",
        "# Better performance with the tf.data API\n",
        "# Reference: https://www.tensorflow.org/guide/data_performance\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kAR4k4uHXkP"
      },
      "source": [
        "\n",
        "    \n",
        "## **PART 1 [35 points]: Language Modeling using ngrams**\n",
        "<br />    \n",
        "\n",
        "You have been tasked with developing a language model to complete messages that for some reason arrive incomplete to a disaster response station. Given the delicate situation, you will have to be extra careful. Each word in the sentence conveys a lot of information, and improper handling of the data could cause someone to come to harm. \n",
        "\n",
        "Your language model will be based on bigrams. You'll develop your own sub-word tokenization to analyze disaster messages from multiple natural disasters. All the sentences are translated into English.\n",
        "    \n",
        "    \n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3G-7bWnyHXkQ"
      },
      "source": [
        "### **PART 1: Questions**\n",
        "<br />\n",
        "\n",
        "### **1.1 [5 points] PREPROCESS THE DATASET**\n",
        "<br />\n",
        "\n",
        "**1.1.1** - Read in the dataset `disaster_response_messages_training.csv` and select only the column \"message\" and display the head of the DataFrame.\n",
        "<br /><br />\n",
        "\n",
        "**1.1.2** - Define a function `clean_data` that takes the data frame as input, converts the characters to lower case and removes any non-alphanumeric characters,  adds the start token `<s>` and the end token `</s>` to every sentence (row) in the data frame and returns the processed data frame.\n",
        "<br />\n",
        "**Sample Input:** \"Is there a dog^ on the road?\" <br />\n",
        "**Sample Output:** \"\\<s> is there a dog on the road \\</s>\"\n",
        "<br /><br />\n",
        "\n",
        "\n",
        "**1.1.3** - Split the dataset into train and test sets. The proportion should be 0.95 and 0.05, respectively. You will create the language model based on the train set and validate your results on the test set.\n",
        "<br /><br />    \n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "### **1.2 [8 points] TOKENIZE AND COUNT**\n",
        "<br />\n",
        "\n",
        "In this section, you will create three different tokenizers that you will build LM based on. The tokenization functions must divide the text into tokens, count their frequency and return a dictionary with a mapping of token to number.\n",
        "<br />\n",
        "\n",
        "**1.2.1** - Create your own tokenization function ('tokenizer_1') based on whitespace. Pick the top 1000 tokens with the highest frequency to include in the vocabulary. Include the `<UNK>` token for out of the vocabulary (OOV) words.  <br /><br />\n",
        "\n",
        "**1.2.2** - Create a second tokenization function ('tokenizer_2') based on whitespace, but do not limit the vocabulary size.\n",
        "<br /><br />\n",
        "\n",
        "**1.2.3** - Create a third tokenization function ('tokenizer_3') based on sub-words. You have to define a set of common sub-words in the English language, for example, the subtokens _ing_ and _n't_. Here again, do not limit the vocabulary size.\n",
        "\n",
        "In this example, the sentence \"_It is raining outside_\" would be tokenized as [_It_, _is_, _rain_, _ing_, _outside_ ].\n",
        "\n",
        "\n",
        "**Note:** Use words only from the train data to create the vocabulary. Add an additional `<UNK>` token for **1.2.2** and **1.2.3** to handle new words found in the test set. Remove the empty character token as it provides no information\n",
        "<br /><br />\n",
        "    \n",
        "### **1.3 [6 points] CONSTRUCTING BIGRAMS**\n",
        "<br />\n",
        "\n",
        "**1.3.1** - Using each of the tokenizer functions you created, split each sentence into tokens in their numerical representation. \n",
        "<br /><br />\n",
        "\n",
        "**1.3.2** - For each tokenizer, count the occurances of each bigram (w1,w2) in the train dataset and divide them by the total occurences of the first word (w1). This will give you the probability of each bigram.\n",
        "<br /><br />\n",
        "    \n",
        "    \n",
        "### **1.4 [8 points] PREDICTING THE NEXT WORDS**\n",
        "<br />\n",
        "\n",
        "**1.4.1** - Simulate incomplete messages by dividing each sentence of the test set in two. The first $75\\%$ will represent the received message, and the final $25\\%$ will convey the missing information. You will use this dataset to evaluate the predictions of your language model.\n",
        "    \n",
        "For example in the sentence: \"*I will go out on a vacation, now that my semester ended.*\"\n",
        "\n",
        "The first 75% will be \"*I will go out on a vacation, now*\"\n",
        "\n",
        "The last 25% will be \"*that my semeter ended*\"\n",
        "\n",
        "Your aim is to predict the last part by giving your model the first \"part\" of the sentence.\n",
        "\n",
        "\n",
        "**Note:** In an n-gram language model, only the last $n-1$ words are used to make a prediction. \n",
        "\n",
        "For example, for the above sentence, if you are using bigrams, the input to your model would only be \"now\" and you are expected to predict \"that\". \n",
        "<br /><br />\n",
        "\n",
        "    \n",
        "**1.4.2** - Given 5 sentences from the previous question (test set), predict the next word. \n",
        "Append this predicted word to the input sequence and predict the next one. Repeat this process until you reach the 10th predicted token or the end of a sentence (end of a sentence - `</s>`). Compare your results qualitatively with the original sentences. Do the results make sense wrt the context and semantics?\n",
        "\n",
        "Repeat this for all the models built using different tokenization techniques.\n",
        "\n",
        "\n",
        "**Note:** For model 2 and 3 (using tokenizer 2 and 3), if there is an `<UNK>` word as the last word in the test set, predict the next word based on unigram probabilities (calculate this by using word count from Section **1.2**).\n",
        "<br /><br />\n",
        "\n",
        "**1.4.3** - Repeat the same exercise, for all 3 models, but this time, the next token will be sampled from a distribution given by the bigram frequency. Compare and comment on the results?\n",
        "\n",
        "\n",
        "**Hint:** In a model of two bigrams with frequencies 0.7 and 0.3, a deterministic prediction will only predict the first bigram. Sampling from a distribution, will enable the model to predict the second bigram with a probability of 0.3. In this way we can still predict infrequent tokens. \n",
        "<br /><br />\n",
        "    \n",
        "\n",
        "### **1.5 [5 points] EVALUATE THE LANGUAGE MODELS**\n",
        "<br />\n",
        "\n",
        "    \n",
        "**1.5.1** - For each of your models, compute the average perplexity on the test set (These are the complete test messages as tokenized in 1.3.1, **not** the incomplete test messages from 1.4.1). If the tokens of the test set are not present in the train split, define a minimum probability (smoothing). Based on this metric, which model is better?\n",
        "\n",
        "**Note:** Use the bigram probabilities for this. N (from the perplexity formula) - Number of words in the sentence.\n",
        "<br /><br />\n",
        "\n",
        "**1.5.2** - Given the perplexities, which model do you think is better? Why do you think so? Does this reflect the quality of the prediction as seen in part 1.4? \n",
        " What is the effect of UNK words?\n",
        "\n",
        "<br /><br />\n",
        "\n",
        "### **1.6 [3 points] HOMEWORK QUIZ**\n",
        "<br />\n",
        "\n",
        "After attempting this part of the homework, answer the questions on edStem. All the questions depend on this part of the homework and you will not be able to answer them without attempting this part.\n",
        "\n",
        "<br /><br />\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZQGJVcwx6q5"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xf9ieRV5NU38",
        "tags": []
      },
      "source": [
        "## **PART 1: Solutions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g0Zhd1UxHXkR"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "    \n",
        "### **1.1 [5 points] PREPROCESS THE DATASET**\n",
        "<br />\n",
        "\n",
        "**1.1.1** - Read in the dataset `disaster_response_messages_training.csv`. Select only the column \"message\" and display the head of the DataFrame.\n",
        "<br />\n",
        "\n",
        "If you want to download the file you can get it from [here](https://drive.google.com/uc?id=1JRWRywWDZRdxTG1n9H9xTHXNapNdrTOJ&export=download)\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0--qw-2RHXkR"
      },
      "outputs": [],
      "source": [
        "file_path = \"https://drive.google.com/uc?id=1JRWRywWDZRdxTG1n9H9xTHXNapNdrTOJ&export=download\"\n",
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haGkdXvqHXkS"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "    \n",
        "**1.1.2** - Define a function `clean_data` that takes the data frame as input, converts the characters to lower case and removes any non-alphanumeric,  adds the start token `<s>` and the end token `</s>` to every sentence (row) in the data frame and returns the processed data frame.\n",
        "<br />\n",
        "**Sample Input:** \"Is there a dog^ on the road?\" <br />\n",
        "**Sample Output:** \"\\<s> is there a dog on the road \\</s>\"\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nzSqQLiRHXkS"
      },
      "outputs": [],
      "source": [
        "# Your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FE_csbO_HXkS"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "**1.1.3** - Split the dataset into train and test sets. The proportion should be 0.95 and 0.05, respectively. You will create the language model based on the train set and validate your results on the test set.\n",
        "<br /><br />  \n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GqhLV4IDHXkT"
      },
      "outputs": [],
      "source": [
        "# Your code here\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OE3BueW8HXkT"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "### **1.2 [8 points] TOKENIZE AND COUNT**\n",
        "<br />\n",
        "\n",
        "In this section, you will create three different tokenizers and build an LM based on each one of them. The tokenization functions must divide the text into tokens, count their frequency and return a dictionary with a mapping of token to number.\n",
        "<br /><br />\n",
        "    \n",
        "**Note:** Use words only from the train data to create the vocabulary. Add an additional `<UNK>` token for **1.2.2** and **1.2.3** to handle new words found in the test set. Remove the empty character token as it provides no information. \n",
        "<br /><br />\n",
        "\n",
        "**1.2.1** - Create your own tokenization function ('tokenizer_1') based on whitespace. Pick the top 1000 tokens with the highest frequency to include in the vocabulary. Include the `<UNK>` token for out of the vocabulary (OOV) words. \n",
        "<br /><br />\n",
        "    \n",
        "</div> "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XsuySgwlHXkT"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m1xu8CZ-HXkU"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "**1.2.2** - Create a second tokenization function ('tokenizer_2') based on whitespace, but do not limit the vocabulary size.\n",
        "<br /><br />\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XlFMii2QHXkU"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tc8kGcbKHXkU"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "**1.2.3** - Create a third tokenization function ('tokenizer_3') based on sub-words. You have to define a set of common sub-words in the English language, for example, the subtokens _ing_ and _n't_.  Here again, do not limit the vocabulary size.\n",
        "    \n",
        "In this example, the sentence \"_It is raining outside_\" would be tokenized as [_It_, _is_, _rain_, _ing_, _outside_ ].\n",
        "<br /><br />\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VUddvhyfHXkU"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPRIT0WCHXkV"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "### **1.3 [7 points] CONSTRUCTING BIGRAMS**\n",
        "<br />\n",
        "\n",
        "**1.3.1** - Using each of the tokenizer functions you created, split each sentence into tokens in their numerical representation. \n",
        "<br /><br />\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIgSWDWEHXkV"
      },
      "source": [
        "**Tokenizer_1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50JWqq4THXkV"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzPLeOJHHXkV"
      },
      "source": [
        "**Tokenizer_2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6CB5Z1fhHXkV"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p8C1SwPxHXkV"
      },
      "source": [
        "**Tokenizer_3**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9UL8xpqyHXkV"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCxmJSwyHXkW"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "**1.3.2** - For each tokenizer, count the occurances of each bigram (w1,w2) in the train dataset and divide them by the total occurences of the first word (w1). This will give you the probability of each bigram.\n",
        "<br /><br />\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5nNj0_i9HXkW"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8icmHoiHXkW"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "### **1.4 [9 points] PREDICTING THE NEXT WORDS**\n",
        "<br />\n",
        "\n",
        "**1.4.1** - Simulate incomplete messages by dividing each sentence of the test set in two. The first $75\\%$ will represent the received message, and the final $25\\%$ will convey the missing information. You will use this dataset to evaluate the predictions of your language model.\n",
        "    \n",
        "For example in the sentence: \"*I will go out on a vacation, now that my semester ended.*\"\n",
        "\n",
        "The first 75% will be \"*I will go out on a vacation, now*\"\n",
        "\n",
        "The last 25% will be \"*that my semeter ended*\"\n",
        "\n",
        "Your aim is to predict the last part by giving your model the first \"part\" of the sentence.\n",
        "\n",
        "\n",
        "**Note:** In an n-gram language model, only the last $n-1$ words are used to make a prediction. \n",
        "\n",
        "For example, for the above sentence, if you are using bigrams, the input to your model would only be \"now\" and you are expected to predict \"that\". \n",
        "<br /><br />\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCFq-wyOHXkW"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q668WN8JHXkW"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "\n",
        "**1.4.2** - For 5 sentences from the previous question (test set), predict the next word. \n",
        "Append this predicted word to the input sequence and predict the next one. Repeat this process until you reach the 10th predicted token or the end of a sentence (end of a sentence - `</s>`). Compare your results qualitatively with the original sentences. Do the results make sense wrt the context and semantics?\n",
        "\n",
        "Repeat this for all the models built using different tokenization techniques.\n",
        "\n",
        "\n",
        "**Note:** For model 2 and 3  (using tokenizer 2 and 3),, if there is an `<UNK>` word as the last word in the test set, predict the next word based on unigram probabilities (calculate this by using word count from Section **1.2**)\n",
        "<br /><br />\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3XqmN2OAHXkW"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EtpZFMe8HXkX"
      },
      "source": [
        "**Tokenizer1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orucigd_HXkX"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pf7IJrptHXkX"
      },
      "source": [
        "**Tokenizer2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rl3aQERDHXkX"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aV0jyNHFHXkX"
      },
      "source": [
        "**Tokenizer3**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wMUlZeAhHXkX"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ebMOukaHXkY"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "**1.4.3** - Repeat the same exercise, but this time, the next token will be _sampled_ from a distribution given by the bigram frequency. Are the results better?\n",
        "\n",
        "In a model of two bigrams with frequencies 0.7 and 0.3, a deterministic prediction will only predict the first bigram. Sampling from a distribution, will enable the model to predict the second bigram with a probability of 0.3. In this way we can still predict infrequent tokens. \n",
        "<br /><br />\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nw5C2t38HXkY"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ACZ7LVm0HXkY"
      },
      "source": [
        "**Tokenizer_1**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P9pISmymHXkY"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7w_gG_4AHXkY"
      },
      "source": [
        "**Tokenizer_2**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqYkoDt4HXkY"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BB4ESZF9HXkY"
      },
      "source": [
        "**Tokenizer_3**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "88g1jRSMHXkZ"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZcTt2wsHXkZ"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "### **1.5 [6 points] EVALUATE THE LANGUAGE MODELS**\n",
        "<br />\n",
        "\n",
        "    \n",
        "**1.5.1** - For each of your models, compute the average perplexity on the test set (These are the complete test messages as tokenized in 1.3.1, **not** the incomplete test messages from 1.4.1). If the tokens of the test set are not present in the train split, define a minimum probability (smoothing). Based on this metric, which model is better?\n",
        "\n",
        "**Note:** Use the bigram probabilities for this. N - (from the perplexity formula) Number of words in the sentence.\n",
        "<br /><br />\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yt8RfxTJHXkZ"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyHHj10DHXkZ"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "**1.5.2** - Given the perplexities, which model do you think is better? Why do you think so? Does this reflect the quality of the prediction as seen in part 1.4? \n",
        " What is the effect of UNK words?\n",
        "<br /><br />\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSMZ8TueHXkZ"
      },
      "source": [
        "Type your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLx_dIVbwW_y"
      },
      "source": [
        "___\n",
        "___\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5s8tzWckHXkf"
      },
      "source": [
        "## **PART 2 [35 points] : Language Modelling using RNNs**\n",
        "<br />    \n",
        "\n",
        "In this part of the homework, you are to build and train a new language model. For this, we will be using Prof. Protopapas's famous texts which end with `...` for prediction. Here, you will preprocess a data corpus and train your simple RNN network with it. With this network you will try to predict what he meant when he typed `...`. For this task we will use a form of transfer learning: first training a network on the larger dataset, such as IMDB reviews, and then 'fine tuning' the network to the professor's texts.\n",
        "<br /><br />\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWrWbR1eHXkf",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "## **PART 2: Questions**\n",
        "<br />\n",
        "\n",
        "### **2.1 [2 points] PREPROCESS THE DATASET**\n",
        "<br />\n",
        "\n",
        "**2.1.1** - Read in the dataset `imdb.csv`. Create a new dataframe by splitting each review into individual sentences. The sentences can be delimited by different characters such as period and question mark (eroteme). Call this column as `text` in the new dataframe.\n",
        "<br /><br />\n",
        "\n",
        "**2.1.2** - Define a function `clean_data` that takes the new dataframe as input and removes all html tags and non-alphanumeric characters from the dataframe. Additionally, convert all characters to lower case. Remove all the sentences where the number of words is less than 10 and higher than 30. Finally, add the start token `<s>` and the end token `</s>` to every sentence (row) in the dataframe. Return the processed the dataframe. \n",
        "<br /><br />\n",
        "\n",
        "### **2.2 [2 points] TOKENIZE THE DATASET**\n",
        "<br />\n",
        "\n",
        "**2.2.1** - Instantiate a Tokenizer for the dataset using `tensorflow.keras.preprocessing.text.Tokenizer` with a vocabulary size of 5000.  Do **not** use an additional token for unknown, out of vocabulary words (oov). That is, you will **not** have the equivalent of the `<UNK>` token.\n",
        "\n",
        "*Hint:* Remove all filters from the function as we have already perfomed data cleaning. Set the value of filter to be `''`.\n",
        "<br /><br />\n",
        "\n",
        "**2.2.2** - Fit the tokenizer on the dataset and get the sequence representation of each sentence.\n",
        "<br /><br />\n",
        "\n",
        "### **2.3 [10 points] MODELLING THE DATA**\n",
        "<br />\n",
        "\n",
        "**2.3.1** - The first step is to split the dataset into the predictors ($X$) and the response ($Y$). The predictors for each observation (sentence) are all tokens in that sentence _except_ the **last** token. The response for a given sentence is all tokens in that sentence _except_ the **first**. Using `tf.keras.preprocessing.sequence.pad_sequences` post-pad each sequence in $X$ and $Y$ to a length of 30.\n",
        "    \n",
        "**Hint:** You may need to use `tf.convert_to_tensor` on the objects returned by the padding operation. \n",
        "```\n",
        "Example:\n",
        "if token for <s> = 1 and </s> = 2\n",
        "sentence_i = [1, 48, 2498, 22, 16, 4, 4, 1554, 149, 14, 22, 2]\n",
        "x_i = [1,  48,   2498, 22, 16, 4, 4,    1554, 149, 14, 22, 0, ..., 0]\n",
        "y_i = [48, 2498, 22,   16, 4,  4, 1554, 149,  14,  22, 2,  0, ..., 0]\n",
        "```\n",
        "<br /><br />\n",
        "\n",
        "**2.3.2** - Define a simple RNN model that has an embedding layer with an embedding dimension of 300. You can define any number of RNN layers. The output of the RNN model will be a dense layer with size of the vocabulary and softmax activation. Using the functional API here may make it easier to reuse parts of the network later on.\n",
        "<br /><br />\n",
        "\n",
        "**2.3.3** - Train the model with the $X$ and $Y$ data formed in 2.3.1. Use a validation split of 0.2. The choice of number epochs and batch size is left to you.\n",
        "<br /><br />\n",
        "\n",
        "**2.3.4** - Plot the train and validation loss from the training history.\n",
        "<br /><br />\n",
        "\n",
        "### **2.4 [9 points] PREDICTING THE NEXT WORD**\n",
        "<br />\n",
        "\n",
        "**2.4.1** -Read the dataset `pp_text.csv`. Add only the  start token to each line, remove the last word and tokenize it using the tokenizer fit previously. Convert each sentence to a sequence vector and post-pad to a length of 30. This will be the input for the prediction phase.\n",
        "<br /><br />\n",
        "\n",
        "**2.4.2** - For predicting the next word, use the trained RNN model from above. \n",
        "\n",
        "NOTE - Based on your implementation, the output of the RNN model might have to be different from that of your trained network. You can make use of Keras function API for this.\n",
        "<br /><br />\n",
        "\n",
        "**2.4.3** - Choose any sentence from the list of Pavlos' texts to predict the next word. Input this to the RNN model built for prediction and print the predicted word. Try this out with multiple sentences.\n",
        "<br /><br />\n",
        "\n",
        "**2.4.4** - Do you notice any pattern in the predicted words? Do they seem approriate to the context of the texts as you understand it? What do you attribute this discrepency to? How can you resolve it?\n",
        "\n",
        "Answer in less than 150 words.\n",
        "<br /><br />\n",
        "\n",
        "### **2.5 [6 points] TRAINING AND PREDICTING WITH A DIFFERENT DATASET**\n",
        "<br />\n",
        "\n",
        "**2.5.1** - Read the dataset `cleaned_sarcasm.csv`. This dataset has been preprocessed for you, all you need to do is tokenize, convert to sequence and pad it, similar to 2.2.1, 2.2.2 and 2.3.1.\n",
        "<br /><br />\n",
        "\n",
        "**2.5.2** - Train your RNN model with this data and plot the train and validation trace plot. This part is similar to 2.3.2, 2.3.3 and 2.3.4.\n",
        "<br /><br />\n",
        "\n",
        "**2.5.3** - Repeat 2.4.1, 2.4.2 and 2.4.3 with the RNN model trained using the new dataset.\n",
        "<br /><br />\n",
        "\n",
        "**2.5.4** - How do the results with the new dataset compare to the previous ones? Why do you think so? \n",
        "\n",
        "Answer in less than 100 words.\n",
        "<br /><br />\n",
        "    \n",
        "### **2.6 [3 points] COMPLETING THE SENTENCE**\n",
        "<br />\n",
        "\n",
        "**2.6.1** Until now we have predicted a single word for a given sentence. However, what if he meant more than one word when he typed in `...`\n",
        "\n",
        "We will now predict multiple words for each input sentence. To do this we will first predict one word, append this word to the input text and then predict one more with the updated input. Continue doing this for 5 words or until the end token `</s>` (whichever comes first). \n",
        "<br /><br />\n",
        "\n",
        "### **2.7 [3 points] HOMEWORK QUIZ**\n",
        "<br />\n",
        "After attempting this part of the homework, answer the questions on edStem. All the questions depend on this part of the homework and you will not be able to answer them without attempting this part.\n",
        "\n",
        "<br />\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BaQ8lfVowoy3"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "898ka8eIyMSY"
      },
      "source": [
        "## **PART 2: Solutions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfUtlwvKHXkg"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "    \n",
        "### **2.1 [2 points] PREPROCESS THE DATASET**\n",
        "<br />    \n",
        "\n",
        "**2.1.1** - Read in the dataset `imdb.csv`. Create a new dataframe by splitting each review into individual sentences. The sentences can be delimited by different characters such as period and question mark (eroteme). Call this column as `text` in the new dataframe.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B1HuszSR0_C7"
      },
      "outputs": [],
      "source": [
        "# Read the data\n",
        "file_path = \"https://drive.google.com/uc?id=1QDSIaV4iERVgc3b0xkW0u7EuTyQ8vncm&export=download\"\n",
        "data = pd.read_csv(file_path, encoding='latin1')\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1eLvtBYfHXkg"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvCroMOhHXkg"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "**2.1.2** - Define a function `clean_data` that takes the new dataframe as input and removes all html tags and non-alphabetic characters from the dataframe. Additionally, convert all characters to lower case. Remove all the sentences where the number of words is less than 10 and higher than 30. Finally, add the start token `<s>` and the end token `</s>` to every sentence (row) in the dataframe. Return the processed the dataframe. \n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Q14FLh9HXkg"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRMMXqdMHXkh"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "    \n",
        "### **2.2 [2 points] TOKENIZE THE DATASET**\n",
        "    \n",
        "<br />\n",
        "\n",
        "**2.2.1** - Instantiate a Tokenizer for the dataset using `tensorflow.keras.preprocessing.text.Tokenizer` with a vocabulary size of 5000.Do **not** use an additional token for unknown, out of vocabulary words (oov). That is, you will **not** have the equivalent of the `<UNK>` token.\n",
        "\n",
        "*Hint:* Remove all filters from the function as we have already perfomed data cleaning. Set the value of filter to be `''`.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxRYzZAlHXkh"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3FoDsoziHXkh"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "**2.2.2** - Fit the tokenizer on the dataset and get the sequence representation of each sentence.\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfcKfHC8HXkh"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnaKCmlZHXkh"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "    \n",
        "### **2.3 [10 points] MODELLING THE DATA**\n",
        "    \n",
        "**2.3.1** - The first step is to split the dataset into the predictors ($X$) and the response ($Y$). The predictors for each observation (sentence) are all tokens in that sentence _except_ the **last** token. The response for a given sentence is all tokens in that sentence _except_ the **first**. Using `tf.keras.preprocessing.sequence.pad_sequences` post-pad each sequence in $X$ and $Y$ to a length of 30.\n",
        "    \n",
        "```\n",
        "Example:\n",
        "if token for <s> = 1 and </s> = 2\n",
        "sentence_i = [1, 48, 2498, 22, 16, 4, 4, 1554, 149, 14, 22, 2]\n",
        "x_i = [1,  48,   2498, 22, 16, 4, 4,    1554, 149, 14, 22, 0, ..., 0]\n",
        "y_i = [48, 2498, 22,   16, 4,  4, 1554, 149,  14,  22, 2,  0, ..., 0]\n",
        "```\n",
        "**Hint:** You may need to use `tf.convert_to_tensor` on the objects returned by the padding operation. \n",
        "</div>    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TQYWGPhNHXkh"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-GPuG5XHXkh"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "    \n",
        "**2.3.2** - Define a simple RNN model that has an embedding layer with an embedding dimension of 300. You can define any number of RNN layers. The output of the RNN model will be a dense layer with size of the vocabulary and softmax activation. Using the functional API here may make it easier to reuse parts of the network later on.\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyY55B2CHXkh"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnWND6RSHXki"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "**2.3.3** - Train the model with the $X$ and $Y$ data formed in 2.3.1. Use a validation split of 0.2. The choice of number epochs and batch size is left to you.\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GGt185PRHXki"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eTadAXqHXki"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "**2.3.4** - Plot the train and validation loss from the training history.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSjYr4hnHXki",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHZAUp9vHXki"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "### **2.4 [10 points] PREDICTING THE NEXT WORD**\n",
        "    \n",
        "<br />\n",
        "    \n",
        "**2.4.1** - Read the dataset `pp_text.csv`. Add only the  start token to each line, remove the last word and tokenize it using the tokenizer fit previously. Convert each sentence to a sequence vector and post-pad to a length of 30. This will be the input for the prediction phase.\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJTWuX7VHXki"
      },
      "outputs": [],
      "source": [
        "# Read the data\n",
        "file_path = \"https://drive.google.com/uc?id=1sFolPhc31mqvCxC3JxORmur21e28XeA-&export=download\"\n",
        "df_pred = pd.read_csv(file_path)\n",
        "df_pred.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mmEFZ_TeHXki"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3nVrOByOHXki"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "\n",
        "**2.4.2** - Define a simple RNN model with the trained weights of the trained RNN model for predicting the next word. You can make use of Keras function API to reuse the previously written code. The output of this new model is the last element of the RNN output defined earlier.\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YuKcLtqHXkj"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-RC5P3fqHXkj"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "**2.4.3** - Choose any sentence from the list of Prof. Protopapas's texts to predict the next word. Input this to the RNN model built for prediction and print the predicted word. Try this out with multiple sentences.\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1lE9ODjNHXkj"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "appaIgaqHXkj"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "**2.4.4** - Do you notice any pattern in the predicted words? Do they seem approriate to the context of the texts as you understand it? What do you attribute this discrepency to? How can you resolve it?\n",
        "\n",
        "Answer in less than 150 words.\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKGvrS06HXkj"
      },
      "source": [
        "**Type your answer here**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8uKAkAUYHXkj"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "\n",
        "### **2.5 [7 points] TRAINING AND PREDICTING WITH A DIFFERENT DATASET**\n",
        "<br />\n",
        "    \n",
        "**2.5.1** - Read the dataset `cleaned_sarcasm.csv`. This dataset has been preprocessed for you, all you need to do is tokenize, convert to sequence and pad it, similar to 2.2.1, 2.2.2 and 2.3.1.\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8eVJUn-hHXkj"
      },
      "outputs": [],
      "source": [
        "# Read the data\n",
        "file_path = \"https://drive.google.com/uc?id=1kHZIXEcf_t0t2GcxtafF3mL0kuoC4etf&export=download\"\n",
        "df = pd.read_csv(file_path)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0RMbwMBOHXkj"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWMoTCX2HXkk"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDo-fIiXHXkk"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "**2.5.2** - Train your RNN model with this data and plot the train and validation trace plot. This part is similar to 2.3.2, 2.3.3 and 2.3.4.\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41SMwk6BHXkk"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpqpMMQTHXkk"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "**2.5.3** - Repeat 2.4.1, 2.4.2 and 2.4.3 with the RNN model trained using the new dataset.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ykree6pYHXkk"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h2pfM2rZHXkk"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "**2.5.4** - How do the results with the new dataset compare to the previous ones? Why do you think so? \n",
        "\n",
        "Answer in less than 100 words.\n",
        "    \n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6opgbB2zHXkk"
      },
      "source": [
        "**Type your answer here**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c4LSts_RHXkk"
      },
      "source": [
        "<div class=\"alert alert-block alert-danger\" style=\"color:black;background-color:#E7F4FA\">\n",
        "\n",
        "\n",
        "### **2.6 [4 points] COMPLETING THE SENTENCE**\n",
        "<br />\n",
        "\n",
        "**2.6.1** Until now we have predicted a single word for a given sentence. However, what if he meant more than one word when he typed in `...`\n",
        "\n",
        "We will now predict multiple words for each input sentence. To do this we will first predict one word, append this word to the input text and then predict once more with the updated input. Continue doing this to predict 5 words or until the end token `</s>` (whichever comes first). \n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8VxdEBR0HXkk"
      },
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "HW1_Language_Modeling.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}